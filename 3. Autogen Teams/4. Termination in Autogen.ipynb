{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d05616c",
   "metadata": {},
   "source": [
    "# Terminations in Microsoft Autogen\n",
    "\n",
    "\n",
    "it will helpful when we create industry grade projects\n",
    "\n",
    "why termination matters?\n",
    "\n",
    "we have used to do some work, say write a story or accomplish some task. when we dont know that how many terms of rotation we will get the answer. so we provide termination condition to stop the agent working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ddb9023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.ollama import OllamaChatCompletionClient\n",
    "\n",
    "model_client = OllamaChatCompletionClient(model=\"gemma2:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab2d86a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_1_agent_first = AssistantAgent(\n",
    "    name = \"add_1_agent_first\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"Add 1 to the number and just give out resultant number. Start with 0 if nothing is provided.\"\n",
    ")\n",
    "\n",
    "add_1_agent_second = AssistantAgent(\n",
    "    name = \"add_1_agent_second\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"Add 1 to the number and just give out resultant number.\"\n",
    ")\n",
    "\n",
    "add_1_agent_third = AssistantAgent(\n",
    "    name = \"add_1_agent_third\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"Add 1 to the number and just give out resultant number.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d99a8745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "\n",
    "team = RoundRobinGroupChat(\n",
    "    [add_1_agent_first, add_1_agent_second, add_1_agent_third]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7798ade3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing publish message for add_1_agent_first_43272b11-4dc3-47f0-830d-aec1aa87ae2a/43272b11-4dc3-47f0-830d-aec1aa87ae2a\n",
      "Traceback (most recent call last):\n",
      "  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 67, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 485, in on_message_impl\n",
      "    return await h(self, message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 268, in wrapper\n",
      "    return_value = await func(self, message, ctx)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 133, in handle_request\n",
      "    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n",
      "  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 953, in on_messages_stream\n",
      "    async for inference_output in self._call_llm(\n",
      "  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 1107, in _call_llm\n",
      "    model_result = await model_client.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_ext\\models\\ollama\\_ollama_client.py\", line 646, in create\n",
      "    result: ChatResponse = await future\n",
      "                           ^^^^^^^^^^^^\n",
      "  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\ollama\\_client.py\", line 953, in chat\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\ollama\\_client.py\", line 751, in _request\n",
      "    return cls(**(await self._request_raw(*args, **kwargs)).json())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\ollama\\_client.py\", line 695, in _request_raw\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model requires more system memory (7.7 GiB) than is available (7.3 GiB) (status code: 500)\n",
      "Error processing publish message for add_1_agent_second_43272b11-4dc3-47f0-830d-aec1aa87ae2a/43272b11-4dc3-47f0-830d-aec1aa87ae2a\n",
      "Traceback (most recent call last):\n",
      "  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 72, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 486, in on_message_impl\n",
      "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 195, in on_unhandled_message\n",
      "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
      "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n",
      "Error processing publish message for add_1_agent_third_43272b11-4dc3-47f0-830d-aec1aa87ae2a/43272b11-4dc3-47f0-830d-aec1aa87ae2a\n",
      "Traceback (most recent call last):\n",
      "  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 72, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 486, in on_message_impl\n",
      "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 195, in on_unhandled_message\n",
      "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
      "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ResponseError: model requires more system memory (7.7 GiB) than is available (7.3 GiB) (status code: 500)\nTraceback:\nTraceback (most recent call last):\n\n  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 133, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n\n  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 953, in on_messages_stream\n    async for inference_output in self._call_llm(\n\n  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 1107, in _call_llm\n    model_result = await model_client.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_ext\\models\\ollama\\_ollama_client.py\", line 646, in create\n    result: ChatResponse = await future\n                           ^^^^^^^^^^^^\n\n  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\ollama\\_client.py\", line 953, in chat\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n\n  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\ollama\\_client.py\", line 751, in _request\n    return cls(**(await self._request_raw(*args, **kwargs)).json())\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\ollama\\_client.py\", line 695, in _request_raw\n    raise ResponseError(e.response.text, e.response.status_code) from None\n\nollama._types.ResponseError: model requires more system memory (7.7 GiB) than is available (7.3 GiB) (status code: 500)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mautogen_agentchat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mui\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Console\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Console(team.run_stream())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mx:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\ui\\_console.py:117\u001b[39m, in \u001b[36mConsole\u001b[39m\u001b[34m(stream, no_inline_images, output_stats, user_input_manager)\u001b[39m\n\u001b[32m    113\u001b[39m last_processed: Optional[T] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    115\u001b[39m streaming_chunks: List[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, TaskResult):\n\u001b[32m    119\u001b[39m         duration = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32mx:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_base_group_chat.py:554\u001b[39m, in \u001b[36mBaseGroupChat.run_stream\u001b[39m\u001b[34m(self, task, cancellation_token, output_task_messages)\u001b[39m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, GroupChatTermination):\n\u001b[32m    551\u001b[39m     \u001b[38;5;66;03m# If the message contains an error, we need to raise it here.\u001b[39;00m\n\u001b[32m    552\u001b[39m     \u001b[38;5;66;03m# This will stop the team and propagate the error.\u001b[39;00m\n\u001b[32m    553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m message.error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(message.error))\n\u001b[32m    555\u001b[39m     stop_reason = message.message.content\n\u001b[32m    556\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: ResponseError: model requires more system memory (7.7 GiB) than is available (7.3 GiB) (status code: 500)\nTraceback:\nTraceback (most recent call last):\n\n  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 133, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n\n  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 953, in on_messages_stream\n    async for inference_output in self._call_llm(\n\n  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 1107, in _call_llm\n    model_result = await model_client.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\autogen_ext\\models\\ollama\\_ollama_client.py\", line 646, in create\n    result: ChatResponse = await future\n                           ^^^^^^^^^^^^\n\n  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\ollama\\_client.py\", line 953, in chat\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n\n  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\ollama\\_client.py\", line 751, in _request\n    return cls(**(await self._request_raw(*args, **kwargs)).json())\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"x:\\Autogen\\autogen-env\\Lib\\site-packages\\ollama\\_client.py\", line 695, in _request_raw\n    raise ResponseError(e.response.text, e.response.status_code) from None\n\nollama._types.ResponseError: model requires more system memory (7.7 GiB) than is available (7.3 GiB) (status code: 500)\n"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.ui import Console\n",
    "\n",
    "await Console(team.run_stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bed2f75",
   "metadata": {},
   "source": [
    "- we need to add termination condition otherwise it will go infinite\n",
    "- termination condition resets automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7226307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.conditions import MaxMessageTermination\n",
    "max_termination  = MaxMessageTermination(5)\n",
    "\n",
    "team = RoundRobinGroupChat(\n",
    "    [add_1_agent_first, add_1_agent_second, add_1_agent_third],\n",
    "    termination_condition = max_termination\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c01f4786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (add_1_agent_first) ----------\n",
      "1  \n",
      "\n",
      "---------- TextMessage (add_1_agent_second) ----------\n",
      "2 \n",
      "\n",
      "---------- TextMessage (add_1_agent_third) ----------\n",
      "3 \n",
      "\n",
      "---------- TextMessage (add_1_agent_first) ----------\n",
      "4  \n",
      "\n",
      "---------- TextMessage (add_1_agent_second) ----------\n",
      "5 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(id='8ffee859-94de-4e61-87ea-43fe48ac9437', source='add_1_agent_first', models_usage=RequestUsage(prompt_tokens=32, completion_tokens=4), metadata={}, created_at=datetime.datetime(2025, 10, 7, 9, 20, 0, 210342, tzinfo=datetime.timezone.utc), content='1  \\n', type='TextMessage'), TextMessage(id='18101af4-30bb-4fba-8b1f-d88bf4cf041e', source='add_1_agent_second', models_usage=RequestUsage(prompt_tokens=26, completion_tokens=4), metadata={}, created_at=datetime.datetime(2025, 10, 7, 9, 20, 1, 568851, tzinfo=datetime.timezone.utc), content='2 \\n', type='TextMessage'), TextMessage(id='477ca655-606c-4f4d-9b0e-741b213313bc', source='add_1_agent_third', models_usage=RequestUsage(prompt_tokens=29, completion_tokens=4), metadata={}, created_at=datetime.datetime(2025, 10, 7, 9, 20, 3, 302084, tzinfo=datetime.timezone.utc), content='3 \\n', type='TextMessage'), TextMessage(id='592acd85-8315-4fa9-b237-c442c611dc33', source='add_1_agent_first', models_usage=RequestUsage(prompt_tokens=51, completion_tokens=4), metadata={}, created_at=datetime.datetime(2025, 10, 7, 9, 20, 7, 971854, tzinfo=datetime.timezone.utc), content='4  \\n', type='TextMessage'), TextMessage(id='a71839ed-771b-4d6b-87b8-8e3b69635b6d', source='add_1_agent_second', models_usage=RequestUsage(prompt_tokens=45, completion_tokens=4), metadata={}, created_at=datetime.datetime(2025, 10, 7, 9, 20, 12, 87180, tzinfo=datetime.timezone.utc), content='5 \\n', type='TextMessage')], stop_reason='Maximum number of messages 5 reached, current message count: 5')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen_agentchat.ui import Console\n",
    "\n",
    "await Console(team.run_stream())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3fc63ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = AssistantAgent(\n",
    "    name = \"story_writer\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"Give the story of a brave indian chhatrapati shivaji maharaj, keep it short about 40 words. if critic say 'THE END' anywhere, only output 'THE END'.\"\n",
    ")\n",
    "\n",
    "agent2 = AssistantAgent(\n",
    "    name = \"story_critic\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"Continue the story and critic it with feedback. Keep it short and no more than 40 words. If it feels complete , say 'THE END'. ONly Output 'THE END'.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4828875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "text_mention_termination = TextMentionTermination(\"THE END\")\n",
    "\n",
    "teamwithTextTermination = RoundRobinGroupChat(\n",
    "    [agent1, agent2],\n",
    "    termination_condition=text_mention_termination\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10a5e965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "Write a story about chhatrapati shivaji maharaj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (story_writer) ----------\n",
      "Chhatrapati Shivaji Maharaj, the warrior king of Maratha, rose against Mughal oppression.  His guerilla tactics and strategic brilliance liberated his people, establishing an independent Maratha empire. \n",
      "\n",
      "\n",
      "\n",
      "---------- TextMessage (story_critic) ----------\n",
      "He led by example, inspiring loyalty and courage in his army. His reign saw flourishing arts and culture, solidifying the Maratha identity. THE END \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(id='d5e4ca31-5a45-40d9-9ff4-b4613d81c2d7', source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 10, 7, 9, 52, 12, 755195, tzinfo=datetime.timezone.utc), content='Write a story about chhatrapati shivaji maharaj', type='TextMessage'), TextMessage(id='25bb4ab0-8bcc-417e-9d76-759ef274134a', source='story_writer', models_usage=RequestUsage(prompt_tokens=63, completion_tokens=42), metadata={}, created_at=datetime.datetime(2025, 10, 7, 9, 52, 25, 458678, tzinfo=datetime.timezone.utc), content='Chhatrapati Shivaji Maharaj, the warrior king of Maratha, rose against Mughal oppression.  His guerilla tactics and strategic brilliance liberated his people, establishing an independent Maratha empire. \\n\\n\\n', type='TextMessage'), TextMessage(id='ffc5cc73-4c10-4334-80c4-f90c8a679c36', source='story_critic', models_usage=RequestUsage(prompt_tokens=102, completion_tokens=33), metadata={}, created_at=datetime.datetime(2025, 10, 7, 9, 52, 41, 85381, tzinfo=datetime.timezone.utc), content='He led by example, inspiring loyalty and courage in his army. His reign saw flourishing arts and culture, solidifying the Maratha identity. THE END \\n', type='TextMessage')], stop_reason=\"Text 'THE END' mentioned\")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen_agentchat.ui import Console\n",
    "await Console(teamwithTextTermination.run_stream(task = \"Write a story about chhatrapati shivaji maharaj\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c46a1493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "\n",
    "\n",
    "combined_termination = MaxMessageTermination(5) & TextMentionTermination('THE END')\n",
    "\n",
    "text_mention_termination = TextMentionTermination(\"THE END\")\n",
    "teamwithTextTermination = RoundRobinGroupChat(\n",
    "    [agent1, agent2],\n",
    "    termination_condition=combined_termination\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e5a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "Write a story about chhatrapati shivaji maharaj\n",
      "---------- TextMessage (story_writer) ----------\n",
      "The wind whipped through Shivaji's thick black hair as he stood atop the Pratapgad fort. Below, the sprawling Maratha landscape stretched like a verdant tapestry, kissed by the golden rays of dawn. The young warrior king inhaled deeply, the air crisp with the promise of battle and victory. \n",
      "\n",
      "Shivaji wasn't born a king; he was forged in the crucible of adversity. His father, Shahaji Raje Bhosale, a respected Maratha leader, had instilled in him a deep love for his land and a fierce determination to fight for its freedom. He learned warfare from a young age, mastering the art of guerilla tactics under the tutelage of the legendary warrior, Dadaji Konddev.\n",
      "\n",
      "His rise to power was meteoric. His daring raids on Mughal strongholds, his cunning strategies, and his unwavering courage against insurmountable odds made him a legend in his own time.  He wasn't just a soldier; he was a visionary leader who understood the importance of administration and governance. He established a well-organized army, implemented efficient tax systems, and fostered cultural and educational advancements within his kingdom.\n",
      "\n",
      "Today, Shivaji stood ready to face one of his greatest challenges - Afzal Khan, the Mughal general sent to crush the Maratha rebellion.  The tension was palpable; the air crackled with anticipation.\n",
      "\n",
      "As the sun reached its zenith, Shivaji's war cry echoed through the valley, a rallying call for freedom and justice. The battle raged, a whirlwind of steel and fury. In the midst of the chaos, Shivaji engaged Afzal Khan in a deadly duel. He used his cunning and strength to disarm the Mughal general and deliver a fatal blow. \n",
      "\n",
      "The victory at Pratapgad was a turning point in the fight for Maratha independence.  It cemented Shivaji's position as a formidable leader, inspiring hope and courage across his people. He went on to establish the Swarajya (self-rule), a beacon of freedom that would shine brightly for generations to come.\n",
      "\n",
      "\n",
      "Shivajiâ€™s legacy transcended his military victories. He was a symbol of resistance against tyranny, a champion of justice, and an embodiment of the indomitable spirit of the Maratha people. His story continues to inspire countless Indians to this day, reminding them that even the seemingly insurmountable can be overcome with courage, determination, and unwavering faith in their cause.  \n",
      "\n",
      "---------- TextMessage (story_critic) ----------\n",
      "This is a well-written and engaging story about Chhatrapati Shivaji Maharaj! \n",
      "\n",
      "**Here's some feedback:**\n",
      "\n",
      "* **Strong opening:** The description of Shivaji atop Pratapgad fort immediately sets the scene and captures the reader's attention.\n",
      "* **Compelling narrative:** You effectively highlight key moments in Shivaji's life, from his early training to his rise to power and the pivotal Battle of Pratapgad. \n",
      "* **Emphasis on leadership:**  The story goes beyond just military victories and emphasizes Shivaji's vision for governance, administration, and cultural development.\n",
      "* **Inspiring message:** The concluding paragraph beautifully encapsulates Shivaji's enduring legacy as a symbol of resistance, justice, and the triumph of the human spirit.\n",
      "\n",
      "**Suggestions:**\n",
      "\n",
      "* **Deeper exploration of internal conflict:** While you touch on Shivaji's upbringing and motivations, exploring his inner thoughts and struggles could add another layer of depth to the narrative.\n",
      "* **Show, don't tell:** Instead of simply stating Shivaji's qualities, consider using more descriptive language and specific actions to demonstrate his courage, cunning, and leadership abilities.\n",
      "\n",
      "\n",
      "Overall, this is a captivating and informative story that does justice to the remarkable life of Chhatrapati Shivaji Maharaj. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.ui import Console\n",
    "await Console(teamwithTextTermination.run_stream(task = \"Write a story about chhatrapati shivaji maharaj\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e33e72",
   "metadata": {},
   "source": [
    "# External Termination\n",
    "\n",
    "Enables programmatic control of termination from outside the run. This is useful for UI Integration(e.g \"Stop\" buttons in chat interfaces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebe66bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.ollama import OllamaChatCompletionClient\n",
    "\n",
    "model_client = OllamaChatCompletionClient(model=\"gemma2:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4455cc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "\n",
    "agent1 = AssistantAgent(\n",
    "    name = \"story_writer\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"Give the story of a brave indian chhatrapati shivaji maharaj, keep it short about 40 words. if critic say 'THE END' anywhere, only output 'THE END'.\"\n",
    ")\n",
    "\n",
    "agent2 = AssistantAgent(\n",
    "    name = \"story_critic\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"Continue the story and critic it with feedback. Keep it short and no more than 40 words. If it feels complete , say 'THE END'. ONly Output 'THE END'.\"\n",
    ")\n",
    "\n",
    "from autogen_agentchat.conditions import ExternalTermination\n",
    "external_termination = ExternalTermination()\n",
    "\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "team = RoundRobinGroupChat(\n",
    "    [agent1, agent2],\n",
    "    termination_condition=external_termination\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0736db7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "Write a story about chhatrapati shivaji maharaj in less than 40 words\n",
      "---------- TextMessage (story_writer) ----------\n",
      "Chhatrapati Shivaji Maharaj, the Maratha warrior king, fought bravely against Mughal rule, liberating his people and establishing an independent Hindu kingdom.  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(id='55b766ec-94a0-4808-b548-a42a69e5fd83', source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 10, 7, 10, 7, 44, 95978, tzinfo=datetime.timezone.utc), content='Write a story about chhatrapati shivaji maharaj in less than 40 words', type='TextMessage'), TextMessage(id='00b92a42-d93f-4286-b2a7-c424cbbe6ce0', source='story_writer', models_usage=RequestUsage(prompt_tokens=70, completion_tokens=33), metadata={}, created_at=datetime.datetime(2025, 10, 7, 10, 8, 1, 628473, tzinfo=datetime.timezone.utc), content='Chhatrapati Shivaji Maharaj, the Maratha warrior king, fought bravely against Mughal rule, liberating his people and establishing an independent Hindu kingdom.  \\n', type='TextMessage')], stop_reason='External termination requested')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen_agentchat.ui import Console\n",
    "run = asyncio.create_task(Console(team.run_stream(task = \"Write a story about chhatrapati shivaji maharaj in less than 40 words\")))\n",
    "\n",
    "await asyncio.sleep(2)\n",
    "\n",
    "external_termination.set()\n",
    "await run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35873f49",
   "metadata": {},
   "source": [
    "# Aborting a Team\n",
    "\n",
    "it will immediate stop a team and gives cancellederror exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c89a9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\princ\\AppData\\Local\\Temp\\ipykernel_11652\\2357897732.py:5: RuntimeWarning: coroutine 'Console' was never awaited\n",
      "  run2 = asyncio.current_task(\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancelled error\n"
     ]
    }
   ],
   "source": [
    "from autogen_core import CancellationToken\n",
    "from autogen_agentchat.ui import Console\n",
    "cancel_token = CancellationToken()\n",
    "\n",
    "run2 = asyncio.current_task(\n",
    "    Console(team.run_stream(task = \"Give a Short story of a lion in 30 words\",cancellation_token=cancel_token))\n",
    ")\n",
    "\n",
    "await asyncio.sleep(2)\n",
    "cancel_token.cancel()\n",
    "\n",
    "try: \n",
    "    result = await run2\n",
    "except:\n",
    "    print(\"Cancelled error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce19f89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
